# -*- coding: utf-8 -*-
"""Unet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yxwkPg16o86dFX6a4aSpcAYTkAIfkpWk
"""

import numpy as np
import  matplotlib.pyplot as plt
import math

import torch,torchvision
from torch.utils.data import TensorDataset,DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

Device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class Self_attention(nn.Module):

  def __init__(self,dmodel,heads,mask=None):
    super(Self_attention,self).__init__()

    self.wq=nn.Linear(dmodel,dmodel)
    self.wk=nn.Linear(dmodel,dmodel)
    self.wv=nn.Linear(dmodel,dmodel)
    self.wo=nn.Linear(dmodel,dmodel)
    self.h=heads
    mask = self.register_buffer("mask", mask)
    self.d=nn.Dropout(0.1)


  def transform(self,x):
    B,T,C=x.shape
    x=x.view(B,T,self.h,C//self.h)
    x=x.transpose(1,2)
    return x

  def forward(self,x):
    B,D,W,H=x.shape
    x=x.view(B,W*H,D)
    B,T,C=x.shape
    q,k,v=self.wq(x),self.wk(x),self.wv(x)
    q,k,v=self.transform(q),self.transform(k),self.transform(v)

    self.mask=torch.ones(T,T).to(Device)
    self.mask=torch.tril(self.mask).detach()

    atten=torch.matmul(q,k.transpose(2,3))
    atten = atten.masked_fill(self.mask[:T,:T] == 0, float('-inf'))
    atten=F.softmax(atten,dim=-1)
    atten=torch.matmul(atten,v)
    atten=atten.transpose(1,2)
    atten=(atten.contiguous()).view(B,T,C)

    out=self.d(self.wo(atten))
    out=out.view(B,D,W,H)
    return out

'''tensor=torch.randn(2,4,4)
cross_att=MHMA(4,4,4,4,2)
x=cross_att(tensor)'''

class Residual_block(nn.Module):
  def __init__(self,in_channels,kernel=3,stride=1,atten=False):
    super(Residual_block,self).__init__()

    self.conv1=nn.Conv2d(in_channels,in_channels,kernel,stride,stride*(kernel//2))
    self.conv2=nn.Conv2d(in_channels,in_channels,kernel,stride,stride*(kernel//2))
    self.at=False
    if atten:
      self.at=True
      self.atten=Self_attention(in_channels,in_channels//2)
      if in_channels//4>0:
        self.inn=in_channels//4
      else:
        self.inn=1
      self.gn1=nn.GroupNorm(self.inn,in_channels)
    self.drop=nn.Dropout(0.1)

  def positional_encoding(self,x,t=0):
    b,d,c,c=x.shape
    pos=torch.zeros(c*c).to(Device)
    for i in range(c*c):
      if i%2==0:
        denom=math.pow(10000,(2*i)/(c*c))
        pos[i]=math.sin(t/denom)
      else:
        denom=math.pow(10000,(2*(i-1))/(c*c))
        pos[i]=math.cos(t/denom)
    pos=pos.view(c,c)
    return pos

  def forward(self,x,t=0):
    x=x+self.positional_encoding(x,t)
    x=F.silu(self.conv1(x))
    if self.at:
      x=self.gn1(self.atten(x)+x)
    x=self.drop(F.silu(self.conv2(x)))
    return x

'''res=Residual_block(8,5,1,True)
x=torch.randn(2,8,5,5)
res(x)'''

class Dblock(nn.Module):
  def __init__(self,in_channels,sample_factor=2,kernel=3,stride=2,atten=False):
    super(Dblock,self).__init__()

    self.conv1=nn.Conv2d(in_channels,in_channels*sample_factor,kernel,stride)
    self.residual=Residual_block(in_channels,atten=atten)
    if in_channels//4>0:
      self.inn=in_channels//4
    else:
      self.inn=1
    self.gn=nn.GroupNorm(self.inn,in_channels)
    

  def forward(self,x,t=0):
    return self.conv1(self.gn(self.residual(x,t)+x))

'''res=Dblock(1)
x=torch.randn(2,1,8,8)
res(x).shape'''

class Ublock(nn.Module):
  def __init__(self,in_channels,sample_factor=2,kernel=3,stride=2,atten=False):
    super(Ublock,self).__init__()

    self.conv1=nn.ConvTranspose2d(in_channels,in_channels//sample_factor,kernel,stride)
    self.residual=Residual_block(in_channels,atten=atten)
    if in_channels//4>0:
      self.inn=in_channels//4
    else:
      self.inn=1
    self.gn=nn.GroupNorm(self.inn,in_channels)

  def forward(self,x,t=0):
    return self.conv1(self.gn(self.residual(x,t)+x))

class Unet(nn.Module):
  def __init__(self,in_channels,f_initial,sample_factor=2,kernel=3):
    super(Unet,self).__init__()

    self.conv1=nn.Conv2d(in_channels,f_initial,kernel,padding=1)
    self.conv2=nn.Conv2d(f_initial,in_channels,kernel,padding=1)
    self.dblocks = nn.ModuleList([Dblock(f_initial * (2**i), atten=(i > 1)) for i in range(0,3)])
    self.ublocks = nn.ModuleList([Ublock(f_initial * (2**i), atten=(i > 2)) for i in range(3,0,-1)])
    self.residual=list(range(4))
    self.residual_block=Residual_block(f_initial*(2**3),atten=True)
    self.inn=f_initial*(2**3)
    self.gn=nn.GroupNorm(self.inn//4,self.inn)

  def forward(self,x,t=0):
    count=0
    x=F.silu(self.conv1(x))
    #print(x.shape)
    self.residual[0]=x
    for i, dblock in enumerate(self.dblocks):
        self.residual[i+1]=dblock(self.residual[i],t)
        count+=1

    x=self.gn(self.residual_block(self.residual[count],t))

    for i ,ublock in enumerate(self.ublocks):
      #print(x.shape,self.residual[count-i].shape,i)
      x=ublock(x+self.residual[count-i],t)
    x=self.conv2(x)
    return x

unet=Unet(1,16,2)
x=torch.randn(2,1,31,31)
print(unet(x).shape)
'''for i in range(3):
  x=torch.randn(2,1,31,31)
  y=torch.randn(2,1,31,31)

  loss=nn.MSELoss()
  optimizer=optim.Adam(unet.parameters(),lr=0.001)
  pred_x=unet(x)
  loss_val=loss(pred_x,y)

  loss_val.backward(retain_graph=True)
  optimizer.step()
  optimizer.zero_grad()
  print(loss_val.item())'''
